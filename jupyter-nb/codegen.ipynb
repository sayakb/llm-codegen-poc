{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Angular Components and Generate a New Component\n",
    "\n",
    "In this notebook, we will use the \"Titan Text G1 - Premier\" foundational model and the \"Titan Embeddings G1 - Text\" embeddings model to analyze the code structure of Angular components and generate a new component based on the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Environment Setup\n",
    "\n",
    "First, we need to set up the environment by importing the necessary libraries and creating a service client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Load and Analyze Angular Components\n",
    "\n",
    "Next, we will load the Angular components from the specified directories and use the embeddings model to analyze their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "def load_all_files_recursive(directory):\n",
    "    documents = []\n",
    "    subfolders = set()  # Set to store unique subfolder names\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # Skip hidden subfolders\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            try:\n",
    "                loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "                loaded_docs = loader.load()\n",
    "                # Filter out empty documents\n",
    "                non_empty_docs = [doc for doc in loaded_docs if doc.page_content.strip()]\n",
    "                documents.extend(non_empty_docs)\n",
    "                print(f\"Parsed file {filepath}\")\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Skipping binary file or file with unknown encoding: {filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {filepath}: {e}\")\n",
    "\n",
    "        # Add subfolder names to the set\n",
    "        for subfolder in dirs:\n",
    "            subfolders.add(os.path.relpath(os.path.join(root, subfolder), directory))\n",
    "    return documents, \", \".join(sorted(subfolders))\n",
    "\n",
    "# Load ALL files recursively\n",
    "documents, subfolders = load_all_files_recursive(\"../src/app\")\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "# Create embeddings\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_client)\n",
    "\n",
    "# Create vector store\n",
    "vectorstore_faiss = FAISS.from_documents(documents, embedding=br_embeddings)\n",
    "print(\"Vector store created.\")\n",
    "\n",
    "# List all subfolders\n",
    "print(f\"\\nSubfolders: {subfolders}\")\n",
    "\n",
    "# List all documents loaded to the vector store\n",
    "print(\"\\n--- Documents in Vector Store ---\")\n",
    "for doc in documents:\n",
    "    print(f\"Document Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "print(\"--- End of Documents in Vector Store ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Generate a New Component\n",
    "\n",
    "Now, we will use the foundational model to generate a new Angular component based on the analysis stored in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# Initialize chat model\n",
    "chat_model = ChatBedrock(model_id=\"amazon.titan-text-premier-v1:0\", client=bedrock_client)\n",
    "\n",
    "# Define system prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for generating Angular components. \"\n",
    "    \"Generate the component based on the human prompts. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Define human prompt\n",
    "human_prompt = (\n",
    "    \"Generate a component called with the name lava_flow. \"\n",
    "    \"Use every single file within the \" + subfolders + \" and no other files \"\n",
    "    \"from the context as a boilerplate for generating the component code. Ensure that \"\n",
    "    \"the generated component has every single file that the boilerplate \"\n",
    "    \"components have. Only output the filenames and their contents. \"\n",
    "    \"Do not output anything else.\"\n",
    ")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create retrieval chain\n",
    "retriever = vectorstore_faiss.as_retriever()\n",
    "question_answer_chain = create_stuff_documents_chain(chat_model, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": human_prompt})\n",
    "\n",
    "# Display the generated code in the specified format\n",
    "generated_code = response['answer']\n",
    "\n",
    "# Final generated code\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we successfully analyzed the structure of existing Angular components using the \"Titan Embeddings G1 - Text\" model and stored the analysis in a vector database. We then utilized the \"Titan Text G1 - Premier\" foundational model to generate a new Angular component based on the analyzed data. This approach demonstrates the potential of leveraging advanced language models to assist in software development tasks, such as code generation and component creation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
